Transformer for Englishâ€“Vietnamese Translation

PyTorch implementation of a Transformer model for machine translation from English to Vietnamese.

ğŸ” Giá»›i thiá»‡u

Dá»± Ã¡n nÃ y hiá»‡n thá»±c hÃ³a mÃ´ hÃ¬nh Transformer â€” má»™t kiáº¿n trÃºc dá»±a hoÃ n toÃ n trÃªn cÆ¡ cháº¿ Attention â€” Ä‘á»ƒ dá»‹ch cÃ¢u tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t.
MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« Ä‘áº§u báº±ng PyTorch, bao gá»“m Ä‘áº§y Ä‘á»§ cÃ¡c thÃ nh pháº§n: Encoder, Decoder, Positional Encoding, Multi-Head Attention vÃ  cÆ¡ cháº¿ Masking trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  dá»‹ch tá»± Ä‘á»™ng.

ğŸ§  Kiáº¿n trÃºc Transformer
1. Kiáº¿n trÃºc tá»•ng thá»ƒ

Transformer hoáº¡t Ä‘á»™ng theo mÃ´ hÃ¬nh encoderâ€“decoder, trong Ä‘Ã³:

Encoder mÃ£ hÃ³a cÃ¢u nguá»“n (tiáº¿ng Anh) thÃ nh cÃ¡c vector ngá»¯ nghÄ©a.

Decoder giáº£i mÃ£ cÃ¡c vector nÃ y Ä‘á»ƒ sinh ra cÃ¢u Ä‘Ã­ch (tiáº¿ng Viá»‡t).

Quy trÃ¬nh tá»•ng quÃ¡t:
Input â†’ Token Embedding â†’ Positional Encoding â†’ Encoder Stack â†’ Decoder Stack â†’ Output

2. Multi-Head Attention

CÆ¡ cháº¿ Attention cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a cÃ¢u á»Ÿ má»i vá»‹ trÃ­.
Vá»›i Multi-Head Attention, má»—i "head" há»c má»™t khÃ­a cáº¡nh khÃ¡c nhau cá»§a ngá»¯ cáº£nh:

Head 1: Quan há»‡ ngá»¯ phÃ¡p

Head 2: Quan há»‡ ngá»¯ nghÄ©a

Head 3: ThÃ´ng tin vá»‹ trÃ­

Head 4: Tá»•ng há»£p ngá»¯ cáº£nh

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh hiá»ƒu sÃ¢u hÆ¡n vá» cáº¥u trÃºc ngÃ´n ngá»¯ vÃ  Ã½ nghÄ©a toÃ n cá»¥c.

3. Positional Encoding

VÃ¬ Transformer khÃ´ng dÃ¹ng RNN nÃªn cáº§n bá»• sung thÃ´ng tin thá»© tá»± vá»‹ trÃ­ cho tá»«ng token.
Positional Encoding Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡c hÃ m sin vÃ  cos á»Ÿ cÃ¡c táº§n sá»‘ khÃ¡c nhau, Ä‘áº£m báº£o mÃ´ hÃ¬nh phÃ¢n biá»‡t Ä‘Æ°á»£c thá»© tá»± cÃ¡c tá»« trong cÃ¢u.

4. Encoder Stack

Má»—i lá»›p trong encoder gá»“m hai khá»‘i chÃ­nh:

Multi-Head Self-Attention

Feed Forward Network

Má»—i khá»‘i Ä‘á»u cÃ³ Residual Connection vÃ  Layer Normalization, giÃºp á»•n Ä‘á»‹nh gradient vÃ  cáº£i thiá»‡n kháº£ nÄƒng há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.

5. Decoder Stack

Decoder bao gá»“m ba thÃ nh pháº§n:

Masked Self-Attention: chá»‰ cho phÃ©p mÃ´ hÃ¬nh nhÃ¬n tháº¥y cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ khi sinh tá»« tiáº¿p theo.

Encoderâ€“Decoder Attention: káº¿t há»£p thÃ´ng tin tá»« encoder Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh nguá»“n.

Feed Forward Network: biáº¿n Ä‘á»•i Ä‘áº·c trÆ°ng phi tuyáº¿n tÃ­nh.

CÆ¡ cháº¿ masking Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ trÃ¡nh mÃ´ hÃ¬nh â€œnhÃ¬n trÆ°á»›câ€ cÃ¡c tá»« trong tÆ°Æ¡ng lai khi dá»‹ch.

6. Feed Forward Network

LÃ  máº¡ng hai lá»›p tuyáº¿n tÃ­nh hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p táº¡i tá»«ng vá»‹ trÃ­ token.
GiÃºp mÃ´ hÃ¬nh tÄƒng kháº£ nÄƒng biá»ƒu diá»…n phi tuyáº¿n vÃ  há»c Ä‘Æ°á»£c quan há»‡ phá»©c táº¡p giá»¯a cÃ¡c tá»«.

7. QuÃ¡ trÃ¬nh Dá»‹ch (Inference)

Transformer dá»‹ch cÃ¢u theo phÆ°Æ¡ng phÃ¡p autoregressive â€” sinh tá»«ng tá»« má»™t, dá»±a vÃ o cÃ¡c tá»« Ä‘Ã£ sinh trÆ°á»›c Ä‘Ã³.
VÃ­ dá»¥:

BÆ°á»›c 1: â€œxinâ€ â†’ â€œchÃ oâ€

BÆ°á»›c 2: â€œxin chÃ oâ€ â†’ â€œbáº¡nâ€

BÆ°á»›c 3: â€œxin chÃ o báº¡nâ€ â†’ â€œcÃ³ khá»e khÃ´ngâ€

ğŸ“Š Káº¿t quáº£ Huáº¥n luyá»‡n

Final Training Loss: 0.8670

Final Validation Loss: 0.8911

Training Perplexity: 2.38

Validation Perplexity: 2.44

Best Model: Epoch 19 (val_loss: 0.8843)

Tá»•ng sá»‘ tham sá»‘: 958,299

ğŸ“š Dá»¯ liá»‡u Huáº¥n luyá»‡n

Sá»‘ lÆ°á»£ng cÃ¢u song ngá»¯: 20 cáº·p cÃ¢u Anhâ€“Viá»‡t

Tá»« vá»±ng nguá»“n (English): 72 tá»«

Tá»« vá»±ng Ä‘Ã­ch (Vietnamese): 91 tá»«

âš™ï¸ TÃ­nh nÄƒng & Ká»¹ thuáº­t

Label Smoothing: 0.1

Cosine Annealing Learning Rate

AdamW Optimizer with Weight Decay

Gradient Clipping

CrossEntropyLoss (bá» qua padding)

ğŸš€ Æ¯u Ä‘iá»ƒm cá»§a Transformer

Song song hÃ³a: Xá»­ lÃ½ toÃ n bá»™ chuá»—i cÃ¹ng lÃºc, tÄƒng tá»‘c huáº¥n luyá»‡n.

Hiá»ƒu phá»¥ thuá»™c dÃ i háº¡n: CÆ¡ cháº¿ Self-Attention giÃºp mÃ´ hÃ¬nh náº¯m báº¯t cÃ¡c quan há»‡ xa trong cÃ¢u.

Hiá»‡u quáº£ tÃ­nh toÃ¡n: Giáº£m Ä‘á»™ phá»©c táº¡p so vá»›i RNN truyá»n thá»‘ng.
